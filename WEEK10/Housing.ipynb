{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder,RobustScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "plt.style.use('seaborn-v0_8')\n",
        "print(\"Setup completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG-iMePkdGyS",
        "outputId": "40bdee16-84fb-4246-9f1b-9da9c618fea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('/content/train.csv')\n",
        "df_test = pd.read_csv('/content/test.csv')\n",
        "print(f\"Train shape: {df_train.shape}\")\n",
        "print(f\"Test shape: {df_test.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRML7SJ7dG07",
        "outputId": "790a2bd9-c50d-4c3a-d843-9e4e199e94b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (1460, 81)\n",
            "Test shape: (1459, 80)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_missing_columns(df, threshold=0.4):\n",
        "    \"\"\"Drop columns with missing values above threshold\"\"\"\n",
        "    missing_ratio = df.isna().mean()\n",
        "    missing_columns = missing_ratio[missing_ratio > threshold].index.tolist()\n",
        "    print(f'Dropping {len(missing_columns)} columns with >{threshold*100}% missing values.')\n",
        "    if missing_columns:\n",
        "        print(f'Columns: {missing_columns}')\n",
        "    return df.drop(columns=missing_columns)\n",
        "\n",
        "def drop_zero_columns(df, threshold=0.5):\n",
        "    \"\"\"Drop columns with zeros above threshold\"\"\"\n",
        "    zero_ratio = (df == 0).astype(int).mean()\n",
        "    zero_columns = zero_ratio[zero_ratio > threshold].index.tolist()\n",
        "    print(f'Dropping {len(zero_columns)} columns with >{threshold*100}% zeros.')\n",
        "    if zero_columns:\n",
        "        print(f'Columns: {zero_columns}')\n",
        "    return df.drop(columns=zero_columns)\n",
        "\n",
        "def merge_rare_categories(df):\n",
        "    \"\"\"Merge rare categories into 'Other'\"\"\"\n",
        "    rare_map = {\n",
        "        'Exterior1st': ['BrkComm', 'Stone', 'AsphShn', 'ImStucc', 'CBlock'],\n",
        "        'Exterior2nd': ['ImStucc', 'Brk Cmn', 'Stone', 'AsphShn', 'Other', 'CBlock'],\n",
        "        'ExterQual': ['Fa'],\n",
        "        'Foundation': ['Wood'],\n",
        "        'GarageType': ['CarPort', '2Types'],\n",
        "        'HeatingQC': ['Po'],\n",
        "        'HouseStyle': ['1.5Unf', '2.5Unf', '2.5Fin'],\n",
        "        'LandSlope': ['Sev'],\n",
        "        'LotConfig': ['FR3'],\n",
        "        'LotShape': ['IR3'],\n",
        "        'MSZoning': ['C (all)'],\n",
        "        'Neighborhood': ['Veenker', 'NPkVill', 'Blueste'],\n",
        "        'RoofStyle': ['Flat', 'Gambrel', 'Mansard', 'Shed'],\n",
        "        'SaleCondition': ['Alloca', 'AdjLand']\n",
        "    }\n",
        "    for col, rare_values in rare_map.items():\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].replace(rare_values, 'Other')\n",
        "    return df"
      ],
      "metadata": {
        "id": "3u3z475YdG29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select important features based on correlation and domain knowledge\n",
        "numeric_features = [\n",
        "    'OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF',\n",
        "    '1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt', 'YearRemodAdd',\n",
        "    'GarageYrBlt', 'YrSold'\n",
        "]\n",
        "\n",
        "categorical_features = [\n",
        "    'BsmtExposure', 'BsmtFinType1', 'BsmtQual', 'Exterior1st', 'Exterior2nd',\n",
        "    'ExterQual', 'Foundation', 'GarageFinish', 'GarageType', 'HeatingQC',\n",
        "    'HouseStyle', 'KitchenQual', 'LandSlope', 'LotConfig', 'LotShape',\n",
        "    'MSZoning', 'Neighborhood', 'RoofStyle', 'SaleCondition'\n",
        "]\n",
        "\n",
        "all_features = numeric_features + categorical_features\n",
        "print(f\"Total features selected: {len(all_features)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rNRwxEnej-H",
        "outputId": "cf6acf59-07a7-4361-8b48-cc72963d7f7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total features selected: 31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df, is_train=True):\n",
        "    \"\"\"Complete preprocessing pipeline\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # 1. Fill missing values for specific columns\n",
        "    cols_fill_none = ['GarageType', 'GarageFinish', 'BsmtQual', 'BsmtExposure', 'BsmtFinType1']\n",
        "    for col in cols_fill_none:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna('None')\n",
        "\n",
        "    if 'GarageYrBlt' in df.columns and 'YearBuilt' in df.columns:\n",
        "        df['GarageYrBlt'] = df['GarageYrBlt'].fillna(df['YearBuilt'])\n",
        "\n",
        "    # 2. Merge rare categories\n",
        "    df = merge_rare_categories(df)\n",
        "\n",
        "    # 3. Create new features BEFORE any transformation\n",
        "    if 'YrSold' in df.columns and 'YearBuilt' in df.columns:\n",
        "        df['HouseAge'] = df['YrSold'] - df['YearBuilt']\n",
        "    if 'YrSold' in df.columns and 'YearRemodAdd' in df.columns:\n",
        "        df['RemodAge'] = df['YrSold'] - df['YearRemodAdd']\n",
        "    if 'YrSold' in df.columns and 'GarageYrBlt' in df.columns:\n",
        "        df['GarageAge'] = df['YrSold'] - df['GarageYrBlt']\n",
        "\n",
        "    # 4. Log transform skewed features\n",
        "    cols_log = ['GrLivArea', 'TotalBsmtSF', '1stFlrSF']\n",
        "    for col in cols_log:\n",
        "        if col in df.columns:\n",
        "            df[col] = np.log1p(df[col].clip(lower=0))\n",
        "\n",
        "    # 5. Ordinal encoding for ordinal categorical features\n",
        "    ordinal_maps = {\n",
        "        'BsmtExposure': {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n",
        "        'BsmtFinType1': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n",
        "        'BsmtQual': {'None': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},\n",
        "        'ExterQual': {'Other': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},\n",
        "        'GarageFinish': {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3},\n",
        "        'HeatingQC': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},\n",
        "        'KitchenQual': {'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},\n",
        "        'LandSlope': {'Sev': 0, 'Mod': 1, 'Gtl': 2},\n",
        "        'LotShape': {'IR3': 0, 'IR2': 1, 'IR1': 2, 'Reg': 3}\n",
        "    }\n",
        "\n",
        "    for col, mapping in ordinal_maps.items():\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].map(mapping).fillna(0).astype(int)\n",
        "\n",
        "    # 6. Create quality scores\n",
        "    if 'BsmtQual' in df.columns and 'BsmtExposure' in df.columns and 'BsmtFinType1' in df.columns:\n",
        "        df['BasementScore'] = df['BsmtQual'] + df['BsmtExposure'] + df['BsmtFinType1']\n",
        "    if 'GarageFinish' in df.columns and 'GarageCars' in df.columns:\n",
        "        df['GarageScore'] = df['GarageFinish'] + df['GarageCars']\n",
        "    if 'ExterQual' in df.columns and 'KitchenQual' in df.columns:\n",
        "        df['ExteriorScore'] = df['ExterQual'] + df['KitchenQual']\n",
        "\n",
        "    # 7. Create total square footage\n",
        "    if {'TotalBsmtSF', '1stFlrSF', 'GrLivArea'}.issubset(df.columns):\n",
        "        df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['GrLivArea']\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "f2kAngtcdG5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess both train and test\n",
        "df_train_processed = preprocess_data(df_train, is_train=True)\n",
        "df_test_processed = preprocess_data(df_test, is_train=False)\n",
        "\n",
        "print(\"Preprocessing completed\")\n",
        "print(f\"Train shape: {df_train_processed.shape}\")\n",
        "print(f\"Test shape: {df_test_processed.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU9hhDT5dG8N",
        "outputId": "76f1f6fd-0fb1-4cef-fdb0-59a7f84b039e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing completed\n",
            "Train shape: (1460, 88)\n",
            "Test shape: (1459, 87)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all numeric columns and remaining categorical columns\n",
        "numeric_cols = df_train_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if 'SalePrice' in numeric_cols:\n",
        "    numeric_cols.remove('SalePrice')\n",
        "if 'Id' in numeric_cols:\n",
        "    numeric_cols.remove('Id')\n",
        "\n",
        "categorical_cols = df_train_processed.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"Numeric features: {len(numeric_cols)}\")\n",
        "print(f\"Categorical features: {len(categorical_cols)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cl9Ltht6dG-V",
        "outputId": "81063e45-e502-45e2-8138-b76c2b141637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numeric features: 52\n",
            "Categorical features: 34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#One-hot encoding for nominal categorical features\n",
        "if len(categorical_cols) > 0:\n",
        "    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "    # Fit on train\n",
        "    encoded_train = pd.DataFrame(\n",
        "        encoder.fit_transform(df_train_processed[categorical_cols]),\n",
        "        columns=encoder.get_feature_names_out(categorical_cols),\n",
        "        index=df_train_processed.index\n",
        "    )\n",
        "\n",
        "    # Transform test\n",
        "    encoded_test = pd.DataFrame(\n",
        "        encoder.transform(df_test_processed[categorical_cols]),\n",
        "        columns=encoder.get_feature_names_out(categorical_cols),\n",
        "        index=df_test_processed.index\n",
        "    )\n",
        "\n",
        "    # Combine with numeric features\n",
        "    X_train = pd.concat([\n",
        "        df_train_processed[numeric_cols].reset_index(drop=True),\n",
        "        encoded_train.reset_index(drop=True)\n",
        "    ], axis=1)\n",
        "\n",
        "    X_test = pd.concat([\n",
        "        df_test_processed[numeric_cols].reset_index(drop=True),\n",
        "        encoded_test.reset_index(drop=True)\n",
        "    ], axis=1)\n",
        "else:\n",
        "    X_train = df_train_processed[numeric_cols].copy()\n",
        "    X_test = df_test_processed[numeric_cols].copy()\n",
        "\n",
        "# Target variable (log transform)\n",
        "y_train = np.log1p(df_train['SalePrice'])\n",
        "\n",
        "print(f\"\\nFinal X_train shape: {X_train.shape}\")\n",
        "print(f\"Final X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBNdky9terAK",
        "outputId": "0f39ab30-c2d0-4898-b9c1-bae45ff6a1fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final X_train shape: (1460, 260)\n",
            "Final X_test shape: (1459, 260)\n",
            "y_train shape: (1460,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values with median from training set\n",
        "train_medians = X_train.median()\n",
        "X_train = X_train.fillna(train_medians)\n",
        "X_test = X_test.fillna(train_medians)\n",
        "\n",
        "# Check for any remaining NaN or inf\n",
        "print(f\"Train NaN count: {X_train.isna().sum().sum()}\")\n",
        "print(f\"Test NaN count: {X_test.isna().sum().sum()}\")\n",
        "print(f\"Train Inf count: {np.isinf(X_train.values).sum()}\")\n",
        "print(f\"Test Inf count: {np.isinf(X_test.values).sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mTwBA3nep01",
        "outputId": "5ac39db1-ff66-4579-92e0-f2ff4c9ff14e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train NaN count: 0\n",
            "Test NaN count: 0\n",
            "Train Inf count: 0\n",
            "Test Inf count: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use RobustScaler instead of StandardScaler (better for outliers)\n",
        "scaler = RobustScaler()\n",
        "X_train_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(X_train),\n",
        "    columns=X_train.columns,\n",
        "    index=X_train.index\n",
        ")\n",
        "X_test_scaled = pd.DataFrame(\n",
        "    scaler.transform(X_test),\n",
        "    columns=X_test.columns,\n",
        "    index=X_test.index\n",
        ")\n",
        "\n",
        "print(\"Robust Standardization completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_6DN-T9ep39",
        "outputId": "009013fe-ca47-4d87-e22e-dbb906e3d591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robust Standardization completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== OPTIMIZED ENSEMBLE STACKING =====================\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LassoCV, ElasticNetCV, RidgeCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "# === Utility functions ===\n",
        "def rmse_log(y_log_true, y_log_pred):\n",
        "    return np.sqrt(mean_squared_error(y_log_true, y_log_pred))\n",
        "\n",
        "# === Prepare data ===\n",
        "Xtrn = X_train_scaled.values\n",
        "Xtst = X_test_scaled.values\n",
        "y    = y_train.values\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Containers for OOF and test predictions\n",
        "oof_lasso = np.zeros_like(y, dtype=float)\n",
        "oof_ridge = np.zeros_like(y, dtype=float)\n",
        "oof_enet  = np.zeros_like(y, dtype=float)\n",
        "oof_xgb   = np.zeros_like(y, dtype=float)\n",
        "oof_lgb   = np.zeros_like(y, dtype=float)\n",
        "\n",
        "tst_lasso = np.zeros(Xtst.shape[0], dtype=float)\n",
        "tst_ridge = np.zeros(Xtst.shape[0], dtype=float)\n",
        "tst_enet  = np.zeros(Xtst.shape[0], dtype=float)\n",
        "tst_xgb   = np.zeros(Xtst.shape[0], dtype=float)\n",
        "tst_lgb   = np.zeros(Xtst.shape[0], dtype=float)\n",
        "\n",
        "print(\"\\n=== 10-Fold CV with Multiple Models ===\")\n",
        "\n",
        "# === Cross-validation ===\n",
        "for fold, (tr_idx, val_idx) in enumerate(kf.split(Xtrn, y), 1):\n",
        "    X_tr, X_va = Xtrn[tr_idx], Xtrn[val_idx]\n",
        "    y_tr, y_va = y[tr_idx],    y[val_idx]\n",
        "\n",
        "    # ----- LassoCV -----\n",
        "    lasso = LassoCV(\n",
        "        alphas=np.logspace(-4.5, -2.0, 80),\n",
        "        cv=5,\n",
        "        random_state=42,\n",
        "        max_iter=10000,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    lasso.fit(X_tr, y_tr)\n",
        "    oof_lasso[val_idx] = lasso.predict(X_va)\n",
        "    tst_lasso += lasso.predict(Xtst) / kf.get_n_splits()\n",
        "\n",
        "    # ----- RidgeCV -----\n",
        "    ridge = RidgeCV(\n",
        "        alphas=np.logspace(-2, 3, 100),\n",
        "        cv=5\n",
        "    )\n",
        "    ridge.fit(X_tr, y_tr)\n",
        "    oof_ridge[val_idx] = ridge.predict(X_va)\n",
        "    tst_ridge += ridge.predict(Xtst) / kf.get_n_splits()\n",
        "\n",
        "    # ----- ElasticNetCV -----\n",
        "    enet = ElasticNetCV(\n",
        "        l1_ratio=[0.05, 0.1, 0.3, 0.5, 0.7, 0.85, 0.95, 1.0],\n",
        "        alphas=np.logspace(-4.5, -2.0, 60),\n",
        "        cv=5,\n",
        "        random_state=42,\n",
        "        max_iter=10000,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    enet.fit(X_tr, y_tr)\n",
        "    oof_enet[val_idx] = enet.predict(X_va)\n",
        "    tst_enet += enet.predict(Xtst) / kf.get_n_splits()\n",
        "\n",
        "    # ----- XGBoost (optimized params) -----\n",
        "    dtr  = xgb.DMatrix(X_tr.astype(np.float32), label=y_tr.astype(np.float32))\n",
        "    dva  = xgb.DMatrix(X_va.astype(np.float32), label=y_va.astype(np.float32))\n",
        "    dtst = xgb.DMatrix(Xtst.astype(np.float32))\n",
        "\n",
        "    xgb_params = {\n",
        "        \"eta\": 0.02,\n",
        "        \"max_depth\": 3,\n",
        "        \"subsample\": 0.75,\n",
        "        \"colsample_bytree\": 0.6,\n",
        "        \"min_child_weight\": 3,\n",
        "        \"reg_alpha\": 0.5,\n",
        "        \"reg_lambda\": 2.0,\n",
        "        \"gamma\": 0.1,\n",
        "        \"objective\": \"reg:squarederror\",\n",
        "        \"seed\": 42,\n",
        "        \"eval_metric\": \"rmse\"\n",
        "    }\n",
        "\n",
        "    xgb_model = xgb.train(\n",
        "        xgb_params,\n",
        "        dtr,\n",
        "        num_boost_round=5000,\n",
        "        evals=[(dva, \"valid\")],\n",
        "        early_stopping_rounds=200,\n",
        "        verbose_eval=False\n",
        "    )\n",
        "\n",
        "    oof_xgb[val_idx] = xgb_model.predict(dva, iteration_range=(0, xgb_model.best_iteration + 1))\n",
        "    tst_xgb += xgb_model.predict(dtst, iteration_range=(0, xgb_model.best_iteration + 1)) / kf.get_n_splits()\n",
        "\n",
        "    # ----- LightGBM -----\n",
        "    lgb_params = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'learning_rate': 0.02,\n",
        "        'max_depth': 3,\n",
        "        'num_leaves': 10,\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.7,\n",
        "        'min_child_samples': 20,\n",
        "        'reg_alpha': 0.5,\n",
        "        'reg_lambda': 2.0,\n",
        "        'verbose': -1,\n",
        "        'seed': 42\n",
        "    }\n",
        "\n",
        "    lgb_train = lgb.Dataset(X_tr, y_tr)\n",
        "    lgb_valid = lgb.Dataset(X_va, y_va, reference=lgb_train)\n",
        "\n",
        "    lgb_model = lgb.train(\n",
        "        lgb_params,\n",
        "        lgb_train,\n",
        "        num_boost_round=5000,\n",
        "        valid_sets=[lgb_valid],\n",
        "        callbacks=[lgb.early_stopping(200), lgb.log_evaluation(0)]\n",
        "    )\n",
        "\n",
        "    oof_lgb[val_idx] = lgb_model.predict(X_va, num_iteration=lgb_model.best_iteration)\n",
        "    tst_lgb += lgb_model.predict(Xtst, num_iteration=lgb_model.best_iteration) / kf.get_n_splits()\n",
        "\n",
        "    # --- Fold scores ---\n",
        "    s_lasso = rmse_log(y_va, oof_lasso[val_idx])\n",
        "    s_ridge = rmse_log(y_va, oof_ridge[val_idx])\n",
        "    s_enet  = rmse_log(y_va, oof_enet[val_idx])\n",
        "    s_xgb   = rmse_log(y_va, oof_xgb[val_idx])\n",
        "    s_lgb   = rmse_log(y_va, oof_lgb[val_idx])\n",
        "\n",
        "    print(f\"Fold {fold:>2}: Lasso={s_lasso:.5f} Ridge={s_ridge:.5f} ENet={s_enet:.5f} XGB={s_xgb:.5f} LGB={s_lgb:.5f}\")\n",
        "\n",
        "# === OOF Scores ===\n",
        "print(\"\\n=== OOF Scores (RMSLE) ===\")\n",
        "print(f\"Lasso : {rmse_log(y, oof_lasso):.5f}\")\n",
        "print(f\"Ridge : {rmse_log(y, oof_ridge):.5f}\")\n",
        "print(f\"ENet  : {rmse_log(y, oof_enet):.5f}\")\n",
        "print(f\"XGB   : {rmse_log(y, oof_xgb):.5f}\")\n",
        "print(f\"LGB   : {rmse_log(y, oof_lgb):.5f}\")\n",
        "\n",
        "# === Optimal Stacking Weights (minimize RMSLE on OOF) ===\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "def blend_loss(weights):\n",
        "    weights = weights / weights.sum()\n",
        "    blend = (weights[0] * oof_lasso +\n",
        "             weights[1] * oof_ridge +\n",
        "             weights[2] * oof_enet +\n",
        "             weights[3] * oof_xgb +\n",
        "             weights[4] * oof_lgb)\n",
        "    return rmse_log(y, blend)\n",
        "\n",
        "initial_weights = np.array([0.2, 0.1, 0.2, 0.25, 0.25])\n",
        "bounds = [(0, 1)] * 5\n",
        "result = minimize(blend_loss, initial_weights, method='SLSQP', bounds=bounds)\n",
        "\n",
        "optimal_weights = result.x / result.x.sum()\n",
        "print(f\"\\n=== Optimal Ensemble Weights ===\")\n",
        "print(f\"Lasso: {optimal_weights[0]:.3f}\")\n",
        "print(f\"Ridge: {optimal_weights[1]:.3f}\")\n",
        "print(f\"ENet : {optimal_weights[2]:.3f}\")\n",
        "print(f\"XGB  : {optimal_weights[3]:.3f}\")\n",
        "print(f\"LGB  : {optimal_weights[4]:.3f}\")\n",
        "\n",
        "# Final blended OOF\n",
        "oof_blend = (optimal_weights[0] * oof_lasso +\n",
        "             optimal_weights[1] * oof_ridge +\n",
        "             optimal_weights[2] * oof_enet +\n",
        "             optimal_weights[3] * oof_xgb +\n",
        "             optimal_weights[4] * oof_lgb)\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Blended OOF RMSLE: {rmse_log(y, oof_blend):.5f}\")\n",
        "\n",
        "# === Final Test Predictions ===\n",
        "test_pred_log = (optimal_weights[0] * tst_lasso +\n",
        "                 optimal_weights[1] * tst_ridge +\n",
        "                 optimal_weights[2] * tst_enet +\n",
        "                 optimal_weights[3] * tst_xgb +\n",
        "                 optimal_weights[4] * tst_lgb)\n",
        "\n",
        "test_pred = np.expm1(test_pred_log)\n",
        "\n",
        "# Post-processing\n",
        "train_prices = df_train['SalePrice'].values\n",
        "q1, q3 = np.percentile(train_prices, 25), np.percentile(train_prices, 75)\n",
        "iqr = q3 - q1\n",
        "lo, hi = max(50000, q1 - 1.5*iqr), q3 + 2.0*iqr\n",
        "test_pred = np.clip(test_pred, lo, hi)\n",
        "\n",
        "# Save submission\n",
        "submission_ensemble = pd.DataFrame({'Id': df_test['Id'], 'SalePrice': test_pred})\n",
        "submission_ensemble.to_csv('submission_optimized_ensemble.csv', index=False)\n",
        "print(\"\\nðŸš€ Saved submission_optimized_ensemble.csv\")\n",
        "print(f\"Price range: ${test_pred.min():.0f} - ${test_pred.max():.0f}\")\n",
        "print(f\"Mean price: ${test_pred.mean():.0f}\")\n",
        "# ======================================================================nos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGrbtHZBnj_d",
        "outputId": "549ff4fd-e794-485f-b9e5-2019d5d46f28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 10-Fold CV with Multiple Models ===\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1102]\tvalid_0's rmse: 0.126354\n",
            "Fold  1: Lasso=0.10645 Ridge=0.11162 ENet=0.10665 XGB=0.12037 LGB=0.12635\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[616]\tvalid_0's rmse: 0.140101\n",
            "Fold  2: Lasso=0.13791 Ridge=0.14501 ENet=0.13937 XGB=0.13892 LGB=0.14010\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[989]\tvalid_0's rmse: 0.0994824\n",
            "Fold  3: Lasso=0.10664 Ridge=0.10904 ENet=0.10768 XGB=0.09906 LGB=0.09948\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2900]\tvalid_0's rmse: 0.117545\n",
            "Fold  4: Lasso=0.12829 Ridge=0.13022 ENet=0.13101 XGB=0.13057 LGB=0.11754\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[247]\tvalid_0's rmse: 0.154803\n",
            "Fold  5: Lasso=0.15301 Ridge=0.14955 ENet=0.15303 XGB=0.15237 LGB=0.15480\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[3223]\tvalid_0's rmse: 0.142489\n",
            "Fold  6: Lasso=0.20300 Ridge=0.19971 ENet=0.20316 XGB=0.14940 LGB=0.14249\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1558]\tvalid_0's rmse: 0.139922\n",
            "Fold  7: Lasso=0.13114 Ridge=0.13437 ENet=0.13072 XGB=0.14299 LGB=0.13992\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2315]\tvalid_0's rmse: 0.112012\n",
            "Fold  8: Lasso=0.10341 Ridge=0.10814 ENet=0.10364 XGB=0.11735 LGB=0.11201\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1641]\tvalid_0's rmse: 0.126472\n",
            "Fold  9: Lasso=0.11624 Ridge=0.12139 ENet=0.11914 XGB=0.13195 LGB=0.12647\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[788]\tvalid_0's rmse: 0.0795655\n",
            "Fold 10: Lasso=0.08716 Ridge=0.08655 ENet=0.08719 XGB=0.07663 LGB=0.07957\n",
            "\n",
            "=== OOF Scores (RMSLE) ===\n",
            "Lasso : 0.13107\n",
            "Ridge : 0.13287\n",
            "ENet  : 0.13186\n",
            "XGB   : 0.12794\n",
            "LGB   : 0.12571\n",
            "\n",
            "=== Optimal Ensemble Weights ===\n",
            "Lasso: 0.269\n",
            "Ridge: 0.000\n",
            "ENet : 0.114\n",
            "XGB  : 0.013\n",
            "LGB  : 0.604\n",
            "\n",
            "ðŸŽ¯ Blended OOF RMSLE: 0.12177\n",
            "\n",
            "ðŸš€ Saved submission_optimized_ensemble.csv\n",
            "Price range: $50000 - $382050\n",
            "Mean price: $176171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ===================== FINAL BOOST PACK (OOF average optimized) =====================\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.linear_model import LassoCV, ElasticNetCV\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "# from scipy.optimize import nnls\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import xgboost as xgb\n",
        "\n",
        "# def rmse_log(y_log_true, y_log_pred):\n",
        "#     return np.sqrt(mean_squared_error(y_log_true, y_log_pred))\n",
        "\n",
        "# def rmse_orig(y_log_true, y_log_pred):\n",
        "#     return np.sqrt(mean_squared_error(np.expm1(y_log_true), np.expm1(y_log_pred)))\n",
        "\n",
        "# # === Data ===\n",
        "# Xtrn = X_train_scaled.values\n",
        "# Xtst = X_test_scaled.values\n",
        "# y    = y_train.values\n",
        "# kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# # Containers\n",
        "# oof_lasso = np.zeros_like(y, dtype=float)\n",
        "# oof_enet  = np.zeros_like(y, dtype=float)\n",
        "# oof_xgb   = np.zeros_like(y, dtype=float)\n",
        "# tst_lasso = np.zeros(Xtst.shape[0], dtype=float)\n",
        "# tst_enet  = np.zeros(Xtst.shape[0], dtype=float)\n",
        "# tst_xgb   = np.zeros(Xtst.shape[0], dtype=float)\n",
        "\n",
        "# print(\"\\n=== 10-Fold OOF training (LassoCV / ElasticNetCV / XGB) ===\")\n",
        "\n",
        "# for fold, (tr_idx, val_idx) in enumerate(kf.split(Xtrn, y), 1):\n",
        "#     X_tr, X_va = Xtrn[tr_idx], Xtrn[val_idx]\n",
        "#     y_tr, y_va = y[tr_idx], y[val_idx]\n",
        "\n",
        "#     # --- Lasso ---\n",
        "#     lasso = LassoCV(\n",
        "#         alphas=np.logspace(-4.5, -2.0, 60),\n",
        "#         cv=5,\n",
        "#         random_state=42,\n",
        "#         max_iter=10000,\n",
        "#         n_jobs=-1\n",
        "#     ).fit(X_tr, y_tr)\n",
        "#     oof_lasso[val_idx] = lasso.predict(X_va)\n",
        "#     tst_lasso += lasso.predict(Xtst) / kf.get_n_splits()\n",
        "\n",
        "#     # --- ElasticNet ---\n",
        "#     enet = ElasticNetCV(\n",
        "#         l1_ratio=[0.05, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n",
        "#         alphas=np.logspace(-4.5, -2.0, 40),\n",
        "#         cv=5,\n",
        "#         random_state=42,\n",
        "#         max_iter=10000,\n",
        "#         n_jobs=-1\n",
        "#     ).fit(X_tr, y_tr)\n",
        "#     oof_enet[val_idx] = enet.predict(X_va)\n",
        "#     tst_enet += enet.predict(Xtst) / kf.get_n_splits()\n",
        "\n",
        "#     # --- XGBoost ---\n",
        "#     dtr = xgb.DMatrix(X_tr.astype(np.float32), label=y_tr)\n",
        "#     dva = xgb.DMatrix(X_va.astype(np.float32), label=y_va)\n",
        "#     dtst = xgb.DMatrix(Xtst.astype(np.float32))\n",
        "#     params = {\n",
        "#         \"eta\": 0.03,\n",
        "#         \"max_depth\": 4,\n",
        "#         \"subsample\": 0.7,\n",
        "#         \"colsample_bytree\": 0.7,\n",
        "#         \"min_child_weight\": 1.0,\n",
        "#         \"reg_alpha\": 1e-3,\n",
        "#         \"reg_lambda\": 1.0,\n",
        "#         \"objective\": \"reg:squarederror\",\n",
        "#         \"seed\": 42\n",
        "#     }\n",
        "#     booster = xgb.train(params, dtr, 10000, evals=[(dva, \"val\")],\n",
        "#                         early_stopping_rounds=200, verbose_eval=False)\n",
        "#     oof_pred = booster.predict(dva, iteration_range=(0, booster.best_iteration + 1))\n",
        "#     tst_pred = booster.predict(dtst, iteration_range=(0, booster.best_iteration + 1))\n",
        "#     oof_xgb[val_idx] = oof_pred\n",
        "#     tst_xgb += tst_pred / kf.get_n_splits()\n",
        "\n",
        "#     print(f\"Fold {fold:>2}: \"\n",
        "#           f\"Lasso={rmse_log(y_va, oof_lasso[val_idx]):.5f}, \"\n",
        "#           f\"ENet={rmse_log(y_va, oof_enet[val_idx]):.5f}, \"\n",
        "#           f\"XGB={rmse_log(y_va, oof_xgb[val_idx]):.5f}\")\n",
        "\n",
        "# # === Tá»•ng há»£p OOF trung bÃ¬nh ===\n",
        "# print(\"\\n=== OOF Results (RMSLE) ===\")\n",
        "# print(f\"Lasso : {rmse_log(y, oof_lasso):.5f}\")\n",
        "# print(f\"ENet  : {rmse_log(y, oof_enet):.5f}\")\n",
        "# print(f\"XGB   : {rmse_log(y, oof_xgb):.5f}\")\n",
        "\n",
        "# # === Blend báº±ng NNLS ===\n",
        "# oof_mat = np.column_stack([oof_lasso, oof_enet, oof_xgb])\n",
        "# w, _ = nnls(oof_mat, y)\n",
        "# w = w / (w.sum() + 1e-9)\n",
        "# print(f\"\\nBlend Weights (NNLS): Lasso={w[0]:.3f}, ENet={w[1]:.3f}, XGB={w[2]:.3f}\")\n",
        "\n",
        "# oof_blend = oof_mat @ w\n",
        "# print(f\"Blended OOF RMSLE : {rmse_log(y, oof_blend):.5f}\")\n",
        "# print(f\"Blended OOF RMSE$ : {rmse_orig(y, oof_blend):,.2f}\")\n",
        "\n",
        "# # === Predict Test ===\n",
        "# tst_mat = np.column_stack([tst_lasso, tst_enet, tst_xgb])\n",
        "# test_pred_log = tst_mat @ w\n",
        "# test_pred = np.expm1(test_pred_log)\n",
        "\n",
        "# # === Háº­u xá»­ lÃ½ nháº¹ ===\n",
        "# train_prices = df_train['SalePrice'].values\n",
        "# q1, q3 = np.percentile(train_prices, 25), np.percentile(train_prices, 75)\n",
        "# iqr = q3 - q1\n",
        "# lo, hi = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
        "# test_pred = np.clip(test_pred, lo, hi)\n",
        "\n",
        "# submission_boost = pd.DataFrame({'Id': df_test['Id'], 'SalePrice': test_pred})\n",
        "# submission_boost.to_csv('submission_boost_final.csv', index=False)\n",
        "# print(\"\\nâœ… Saved submission_boost_final.csv (Stable averaged blend)\")\n",
        "# # ==============================================================================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLXrjRJodHCq",
        "outputId": "d7d5d566-b058-4d99-f179-331dcf871f11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 10-Fold OOF training (LassoCV / ElasticNetCV / XGB) ===\n",
            "Fold  1: Lasso=0.10579, ENet=0.10605, XGB=0.11280\n",
            "Fold  2: Lasso=0.13808, ENet=0.13911, XGB=0.13557\n",
            "Fold  3: Lasso=0.10650, ENet=0.10776, XGB=0.09908\n",
            "Fold  4: Lasso=0.12844, ENet=0.13108, XGB=0.12728\n",
            "Fold  5: Lasso=0.15303, ENet=0.15305, XGB=0.14969\n",
            "Fold  6: Lasso=0.20339, ENet=0.20306, XGB=0.11845\n",
            "Fold  7: Lasso=0.13123, ENet=0.13037, XGB=0.13365\n",
            "Fold  8: Lasso=0.10357, ENet=0.10368, XGB=0.10612\n",
            "Fold  9: Lasso=0.11632, ENet=0.11909, XGB=0.12530\n",
            "Fold 10: Lasso=0.08719, ENet=0.08701, XGB=0.07428\n",
            "\n",
            "=== OOF Results (RMSLE) ===\n",
            "Lasso : 0.13114\n",
            "ENet  : 0.13174\n",
            "XGB   : 0.11996\n",
            "\n",
            "Blend Weights (NNLS): Lasso=0.276, ENet=0.000, XGB=0.724\n",
            "Blended OOF RMSLE : 0.11795\n",
            "Blended OOF RMSE$ : 26,447.34\n",
            "\n",
            "âœ… Saved submission_boost_final.csv (Stable averaged blend)\n"
          ]
        }
      ]
    }
  ]
}